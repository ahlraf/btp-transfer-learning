{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf82ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordninja in c:\\users\\ahlaam\\anaconda3\\lib\\site-packages (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Ahlaam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405c00e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ahlaam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ahlaam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Web scraper for Reddit data\n",
    "\n",
    "# data analysis imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import wordninja\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae6d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user agent\n",
    "headers = {\"User-agent\" : \"wingednova\"} # set user agent to reddit account username\n",
    "url_1 = \"https://www.reddit.com/r/depression.json\"\n",
    "\n",
    "res = requests.get(url_1, headers=headers)\n",
    "res.status_code\n",
    "\n",
    "# scraper function\n",
    "def reddit_scrape(url_string, number_of_scrapes, output_list):\n",
    "    #scraped posts outputted as lists\n",
    "    after = None \n",
    "    for _ in range(number_of_scrapes):\n",
    "        if _ == 0:\n",
    "            print(\"SCRAPING {}\\n--------------------------------------------------\".format(url_string))\n",
    "            print(\"<<<SCRAPING COMMENCED>>>\") \n",
    "            print(\"Downloading Batch {} of {}...\".format(1, number_of_scrapes))\n",
    "        elif (_+1) % 5 ==0:\n",
    "            print(\"Downloading Batch {} of {}...\".format((_ + 1), number_of_scrapes))\n",
    "        \n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            #THIS WILL TELL THE SCRAPER TO GET THE NEXT SET AFTER REDDIT'S after CODE\n",
    "            params = {\"after\": after}             \n",
    "        res = requests.get(url_string, params=params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            output_list.extend(the_json[\"data\"][\"children\"])\n",
    "            after = the_json[\"data\"][\"after\"]\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(random.randint(1,6))\n",
    "    \n",
    "    print(\"<<<SCRAPING COMPLETED>>>\")\n",
    "    print(\"Number of posts downloaded: {}\".format(len(output_list)))\n",
    "    print(\"Number of unique posts: {}\".format(len(set([p[\"data\"][\"name\"] for p in output_list]))))\n",
    "\n",
    "# remove any repeat posts\n",
    "def create_unique_list(original_scrape_list, new_list_name):\n",
    "    data_name_list=[]\n",
    "    for i in range(len(original_scrape_list)):\n",
    "        if original_scrape_list[i][\"data\"][\"name\"] not in data_name_list:\n",
    "            new_list_name.append(original_scrape_list[i][\"data\"])\n",
    "            data_name_list.append(original_scrape_list[i][\"data\"][\"name\"])\n",
    "    #CHECKING IF THE NEW LIST IS OF SAME LENGTH AS UNIQUE POSTS\n",
    "    print(\"LIST NOW CONTAINS {} UNIQUE SCRAPED POSTS\".format(len(new_list_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbfe8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING https://www.reddit.com/r/SuicideWatch.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1251\n",
      "Number of unique posts: 974\n",
      "LIST NOW CONTAINS 974 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/depression.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1230\n",
      "Number of unique posts: 980\n",
      "LIST NOW CONTAINS 980 UNIQUE SCRAPED POSTS\n"
     ]
    }
   ],
   "source": [
    "# scraping r/SuicideWatch data\n",
    "suicide_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/SuicideWatch.json\", 50, suicide_data)\n",
    "\n",
    "suicide_data_unique = []\n",
    "create_unique_list(suicide_data, suicide_data_unique)\n",
    "\n",
    "# add r/SuicideWatch data to dataframe\n",
    "suicide_watch = pd.DataFrame(suicide_data_unique)\n",
    "suicide_watch[\"state\"] = 1\n",
    "suicide_watch.head()\n",
    "\n",
    "# scraping r/depression data\n",
    "depression_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/depression.json\", 50, depression_data)\n",
    "\n",
    "depression_data_unique = []\n",
    "create_unique_list(depression_data, depression_data_unique)\n",
    "\n",
    "# add r/depression data to dataframe\n",
    "depression = pd.DataFrame(depression_data_unique)\n",
    "depression[\"state\"] = 0\n",
    "depression.head()\n",
    "\n",
    "# saving data\n",
    "suicide_watch.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\suicide_watch.csv', index = False)\n",
    "depression.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\depression.csv', index = False)\n",
    "\n",
    "# creating combined CSV\n",
    "depression = pd.read_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\depression.csv')\n",
    "suicide_watch = pd.read_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\suicide_watch.csv')\n",
    "\n",
    "dep_columns = depression[[\"title\", \"selftext\", \"author\",  \"num_comments\", \"state\",\"url\"]]\n",
    "sui_columns = suicide_watch[[\"title\", \"selftext\", \"author\",  \"num_comments\", \"state\",\"url\"]]\n",
    "\n",
    "combined_data = pd.concat([dep_columns, sui_columns],axis=0, ignore_index=True)  \n",
    "combined_data[\"selftext\"].fillna(\"emptypost\",inplace=True)\n",
    "combined_data.head()\n",
    "combined_data.isnull().sum()\n",
    "\n",
    "# saving combined CSV\n",
    "combined_data.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\suicide_vs_depression.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a8bd9",
   "metadata": {},
   "source": [
    "### Scraping r/bulimia, r/AnorexiaNervosa -- test subreddits #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904b0fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING https://www.reddit.com/r/bulimia.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1242\n",
      "Number of unique posts: 992\n",
      "LIST NOW CONTAINS 992 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/AnorexiaNervosa.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1239\n",
      "Number of unique posts: 912\n",
      "LIST NOW CONTAINS 912 UNIQUE SCRAPED POSTS\n"
     ]
    }
   ],
   "source": [
    "# scraping r/bulimia data\n",
    "bulimia_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/bulimia.json\", 50, bulimia_data)\n",
    "\n",
    "bulimia_data_unique = []\n",
    "create_unique_list(bulimia_data, bulimia_data_unique)\n",
    "\n",
    "# add r/bulimia data to dataframe\n",
    "bulimia = pd.DataFrame(bulimia_data_unique)\n",
    "bulimia.head()\n",
    "\n",
    "# scraping r/AnorexiaNervosa data\n",
    "anorexia_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/AnorexiaNervosa.json\", 50, anorexia_data)\n",
    "\n",
    "anorexia_data_unique = []\n",
    "create_unique_list(anorexia_data, anorexia_data_unique)\n",
    "\n",
    "# add r/AnorexiaNervosa data to dataframe\n",
    "anorexia = pd.DataFrame(anorexia_data_unique)\n",
    "anorexia.head()\n",
    "\n",
    "# saving data\n",
    "bulimia.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\bulimia.csv', index = False)\n",
    "anorexia.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\anorexia.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de8979",
   "metadata": {},
   "source": [
    "### Scraping additional subreddits: r/eating_disorders (training), r/EDAnonymousAdults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f801ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING https://www.reddit.com/r/eating_disorders.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1242\n",
      "Number of unique posts: 942\n",
      "LIST NOW CONTAINS 942 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/EdAnonymousAdults.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1247\n",
      "Number of unique posts: 997\n",
      "LIST NOW CONTAINS 997 UNIQUE SCRAPED POSTS\n"
     ]
    }
   ],
   "source": [
    "# scraping r/eating_disorders data\n",
    "ED_train_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/eating_disorders.json\", 50, ED_train_data)\n",
    "\n",
    "ED_train_data_unique = []\n",
    "create_unique_list(ED_train_data, ED_train_data_unique)\n",
    "\n",
    "# add r/eating_disorders data to dataframe\n",
    "ED_train = pd.DataFrame(ED_train_data_unique)\n",
    "ED_train.head()\n",
    "\n",
    "# scraping r/EDAnonymousAdults data\n",
    "EDanon_adults_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/EdAnonymousAdults.json\", 50, EDanon_adults_data)\n",
    "\n",
    "EDanon_adults_data_unique = []\n",
    "create_unique_list(EDanon_adults_data, EDanon_adults_data_unique)\n",
    "\n",
    "# add r/EDAnonymousAdults data to dataframe\n",
    "EDanon_adults = pd.DataFrame(EDanon_adults_data_unique)\n",
    "EDanon_adults.head()\n",
    "\n",
    "# saving data\n",
    "ED_train.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\ED_train.csv', index = False)\n",
    "EDanon_adults.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\EDanon_adults.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76991864",
   "metadata": {},
   "source": [
    "### Scraping recovery subreddits: r/EatingDisorders, r/AnorexiaRecovery, r/BulimiaRecovery, r/fuckeatingdisorders, r/EDRecovery_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ebfba6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING https://www.reddit.com/r/EatingDisorders.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1245\n",
      "Number of unique posts: 893\n",
      "LIST NOW CONTAINS 893 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/AnorexiaRecovery.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1235\n",
      "Number of unique posts: 984\n",
      "LIST NOW CONTAINS 984 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/BulimiaRecovery.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1179\n",
      "Number of unique posts: 141\n",
      "LIST NOW CONTAINS 141 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/fuckeatingdisorders.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1250\n",
      "Number of unique posts: 999\n",
      "LIST NOW CONTAINS 999 UNIQUE SCRAPED POSTS\n",
      "SCRAPING https://www.reddit.com/r/EDRecovery_public.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1240\n",
      "Number of unique posts: 520\n",
      "LIST NOW CONTAINS 520 UNIQUE SCRAPED POSTS\n"
     ]
    }
   ],
   "source": [
    "########## scraping r/EatingDisorders data\n",
    "EDisorders_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/EatingDisorders.json\", 50, EDisorders_data)\n",
    "\n",
    "EDisorders_data_unique = []\n",
    "create_unique_list(EDisorders_data, EDisorders_data_unique)\n",
    "\n",
    "# add r/EatingDisorders data to dataframe\n",
    "EDisorders = pd.DataFrame(EDisorders_data_unique)\n",
    "EDisorders.head()\n",
    "\n",
    "########## scraping r/AnorexiaRecovery data\n",
    "ANRecovery_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/AnorexiaRecovery.json\", 50, ANRecovery_data)\n",
    "\n",
    "ANRecovery_data_unique = []\n",
    "create_unique_list(ANRecovery_data, ANRecovery_data_unique)\n",
    "\n",
    "# add r/AnorexiaRecovery data to dataframe\n",
    "ANRecovery = pd.DataFrame(ANRecovery_data_unique)\n",
    "ANRecovery.head()\n",
    "\n",
    "########## scraping r/BulimiaRecovery\n",
    "BNRecovery_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/BulimiaRecovery.json\", 50, BNRecovery_data)\n",
    "\n",
    "BNRecovery_data_unique = []\n",
    "create_unique_list(BNRecovery_data, BNRecovery_data_unique)\n",
    "\n",
    "# add r/BulimiaRecovery data to dataframe\n",
    "BNRecovery = pd.DataFrame(BNRecovery_data_unique)\n",
    "BNRecovery.head()\n",
    "\n",
    "########## scraping r/fuckeatingdisorders\n",
    "fED_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/fuckeatingdisorders.json\", 50, fED_data)\n",
    "\n",
    "fED_data_unique = []\n",
    "create_unique_list(fED_data, fED_data_unique)\n",
    "\n",
    "# add r/fuckeatingdisorders data to dataframe\n",
    "fED = pd.DataFrame(fED_data_unique)\n",
    "fED.head()\n",
    "\n",
    "########## scraping r/EDRecovery_public\n",
    "EDRecovery_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/EDRecovery_public.json\", 50, EDRecovery_data)\n",
    "\n",
    "EDRecovery_data_unique = []\n",
    "create_unique_list(EDRecovery_data, EDRecovery_data_unique)\n",
    "\n",
    "# add r/EDRecovery_public data to dataframe\n",
    "EDRecovery = pd.DataFrame(EDRecovery_data_unique)\n",
    "EDRecovery.head()\n",
    "\n",
    "# saving data\n",
    "EDisorders.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\recovery\\\\EDisorders.csv', index = False)\n",
    "ANRecovery.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\recovery\\\\ANRecovery.csv', index = False)\n",
    "BNRecovery.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\recovery\\\\BNRecovery.csv', index = False)\n",
    "fED.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\recovery\\\\fED.csv', index = False)\n",
    "EDRecovery.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\recovery\\\\EDRecovery.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b8435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPING https://www.reddit.com/r/CasualConversation.json\n",
      "--------------------------------------------------\n",
      "<<<SCRAPING COMMENCED>>>\n",
      "Downloading Batch 1 of 50...\n",
      "Downloading Batch 5 of 50...\n",
      "Downloading Batch 10 of 50...\n",
      "Downloading Batch 15 of 50...\n",
      "Downloading Batch 20 of 50...\n",
      "Downloading Batch 25 of 50...\n",
      "Downloading Batch 30 of 50...\n",
      "Downloading Batch 35 of 50...\n",
      "Downloading Batch 40 of 50...\n",
      "Downloading Batch 45 of 50...\n",
      "Downloading Batch 50 of 50...\n",
      "<<<SCRAPING COMPLETED>>>\n",
      "Number of posts downloaded: 1235\n",
      "Number of unique posts: 809\n",
      "LIST NOW CONTAINS 809 UNIQUE SCRAPED POSTS\n"
     ]
    }
   ],
   "source": [
    "########## scraping r/CasualConversation data\n",
    "casconv_data = []\n",
    "reddit_scrape(\"https://www.reddit.com/r/CasualConversation.json\", 50, casconv_data)\n",
    "\n",
    "casconv_data_unique = []\n",
    "create_unique_list(casconv_data, casconv_data_unique)\n",
    "\n",
    "# add r/EatingDisorders data to dataframe\n",
    "casconv = pd.DataFrame(casconv_data_unique)\n",
    "casconv[\"state\"] = 2\n",
    "casconv.head()\n",
    "\n",
    "# saving data\n",
    "casconv.to_csv('C:\\\\Users\\\\Ahlaam\\\\Desktop\\\\EP Sem 7\\\\BTP\\\\new-data\\\\control\\\\casconv.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
